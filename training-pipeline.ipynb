{"cells":[{"cell_type":"markdown","metadata":{},"source":["Training pipeline:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UHmxwGGz0Dhw"},"outputs":[],"source":["!pip install transformers[torch]\n","!pip install accelerate\n","!pip install datasets>=2.6.1\n","!pip install librosa\n","!pip install evaluate>=0.30\n","!pip install jiwer"]},{"cell_type":"markdown","metadata":{"id":"5jW625zO9sFo"},"source":["Training and Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S4GYSWlMMeQU"},"outputs":[],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cfVeExLNz3vT"},"outputs":[],"source":["from google.colab import drive\n","drive.flush_and_unmount()\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"svx_nFsOyfc9"},"outputs":[],"source":["from datasets import load_dataset, DatasetDict\n","\n","common_voice = DatasetDict()"]},{"cell_type":"markdown","metadata":{},"source":["Load processed data from Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N_QsUW_Kz3yO"},"outputs":[],"source":["from datasets import load_from_disk\n","common_voice[\"train\"] = load_from_disk(\"/content/drive/MyDrive/common_voice/train\") #.to_iterable_dataset()\n","common_voice[\"test\"] = load_from_disk(\"/content/drive/MyDrive/common_voice/test\") #.to_iterable_dataset()"]},{"cell_type":"markdown","metadata":{"id":"Jc6CAAf21Mkx"},"source":["Use some part of the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ovQ5gdFyHCC"},"outputs":[],"source":["common_voice"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-bcVdsYS_2oR"},"outputs":[],"source":["num_rows_to_keep = int(len(common_voice[\"test\"]) * 0.2)\n","common_voice[\"test\"] = common_voice[\"test\"].select(range(num_rows_to_keep))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iNINn-Sf_2hl"},"outputs":[],"source":["common_voice"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJM3J5yI9Hz7"},"outputs":[],"source":["import torch\n","\n","from dataclasses import dataclass\n","from typing import Any, Dict, List, Union\n","\n","@dataclass\n","class DataCollatorSpeechSeq2SeqWithPadding:\n","    processor: Any\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        # split inputs and labels since they have to be of different lengths and need different padding methods\n","        # first treat the audio inputs by simply returning torch tensors\n","        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n","        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n","\n","        # get the tokenized label sequences\n","        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n","        # pad the labels to max length\n","        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n","\n","        # replace padding with -100 to ignore loss correctly\n","        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n","\n","        # if bos token is appended in previous tokenization step,\n","        # cut bos token here as it's append later anyways\n","        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n","            labels = labels[:, 1:]\n","\n","        batch[\"labels\"] = labels\n","\n","        return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bc77d7bb-f9e2-47f5-b663-30f7a4321ce5"},"outputs":[],"source":["from transformers import WhisperFeatureExtractor\n","\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c7b07f9b-ae0e-4f89-98f0-0c50d432eab6"},"outputs":[],"source":["from transformers import WhisperTokenizer\n","\n","tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Turkish\", task=\"transcribe\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hd820S3kP2G0"},"outputs":[],"source":["from transformers import WhisperProcessor\n","\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Turkish\", task=\"transcribe\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mmnu-fx39rM5"},"outputs":[],"source":["data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"]},{"cell_type":"markdown","metadata":{"id":"I6C6R45Qattk"},"source":["=========="]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OWC_F3SB9rTn"},"outputs":[],"source":["import evaluate\n","\n","metric = evaluate.load(\"wer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kEjZ2jIj9rWU"},"outputs":[],"source":["def compute_metrics(pred):\n","    pred_ids = pred.predictions\n","    label_ids = pred.label_ids\n","\n","    # replace -100 with the pad_token_id\n","    label_ids[label_ids == -100] = tokenizer.pad_token_id\n","\n","    # we do not want to group tokens when computing the metrics\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n","\n","    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n","\n","    return {\"wer\": wer}"]},{"cell_type":"markdown","metadata":{"id":"6ju68ynFL4hx"},"source":["Load a Pre-Trained Checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6v_qhqNYL3uU"},"outputs":[],"source":["from transformers import WhisperForConditionalGeneration\n","\n","model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qpyUQiH2L3w9"},"outputs":[],"source":["model.config.forced_decoder_ids = None\n","model.use_cache = False\n","model.config.suppress_tokens = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TbGkjXjQ9rY5"},"outputs":[],"source":["from transformers import Seq2SeqTrainingArguments\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"/ID2223_Lab2_Whisper\",  # change to a repo name of your choice\n","    per_device_train_batch_size=8,\n","    gradient_accumulation_steps=2,  # increase by 2x for every 2x decrease in batch size\n","    learning_rate=1e-5,\n","    warmup_steps=500,\n","    max_steps=4000,\n","    gradient_checkpointing=True,\n","    fp16=True,\n","    evaluation_strategy=\"steps\",\n","    save_strategy=\"no\",\n","    do_eval = True,\n","    per_device_eval_batch_size=8,\n","    predict_with_generate=True,\n","    generation_max_length=225,\n","    save_steps=500,\n","    eval_steps=500,\n","    logging_steps=25,\n","    report_to=[\"tensorboard\"],\n","    load_best_model_at_end=False,\n","    metric_for_best_model=\"wer\",\n","    greater_is_better=False,\n","    push_to_hub=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"jlPrqJTJnP-I"},"source":["Save checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vlmroeUz2p1r"},"outputs":[],"source":["from transformers import TrainerCallback\n","import os\n","class MyCallback(TrainerCallback):\n","    def on_evaluate(self, args, state, control, model, optimizer, lr_scheduler, **kwargs):\n","      #checkpoint_dir = f\"/content/drive/MyDrive/Whisper_Checkpoints/checkpoint-{state.global_step}\"\n","      checkpoint_dir = os.path.join(args.output_dir, f\"checkpoint-{state.global_step}\")\n","      os.makedirs(checkpoint_dir, exist_ok=True)\n","\n","      # Save model\n","      model.save_pretrained(checkpoint_dir, safe_serialization=True)\n","\n","      # Save training arguments together with the trained model\n","      torch.save(args, os.path.join(checkpoint_dir, \"training_args.bin\"))\n","\n","      # Save state\n","      state.save_to_json(os.path.join(checkpoint_dir, \"trainer_state.json\"))\n","      # Save optimizer\n","      torch.save(optimizer.state_dict(), os.path.join(checkpoint_dir, \"optimizer.pt\"))\n","      # Save scheduler\n","      torch.save(lr_scheduler.state_dict(), os.path.join(checkpoint_dir, \"scheduler.pt\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1bmNFOJGL_nO"},"outputs":[],"source":["from transformers import Seq2SeqTrainer\n","trainer = Seq2SeqTrainer(\n","    args=training_args,\n","    model=model,\n","    train_dataset=common_voice[\"train\"],\n","    eval_dataset=common_voice[\"test\"],\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    tokenizer=processor.feature_extractor,\n","    callbacks=[MyCallback()],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ig4KAzEiL_zY"},"outputs":[],"source":["processor.save_pretrained(training_args.output_dir)"]},{"cell_type":"markdown","metadata":{"id":"SkPmyYlxnNI6"},"source":["Load the saved checkpoint and train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFOrnRTJL_2d"},"outputs":[],"source":["trainer.train(resume_from_checkpoint=True)\n","#trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6arFTGi4MKV5"},"outputs":[],"source":["kwargs = {\n","    \"dataset_tags\": \"mozilla-foundation/common_voice_11_0\",\n","    \"dataset\": \"Common Voice 11.0\",  # a 'pretty' name for the training dataset\n","    \"dataset_args\": \"config: tr, split: test\",\n","    \"language\": \"tr\",\n","    \"model_name\": \"Whisper-Tr-id2223-lab2\",  # a 'pretty' name for our model\n","    \"finetuned_from\": \"openai/whisper-small\",\n","    \"tasks\": \"automatic-speech-recognition\",\n","    \"tags\": \"hf-asr-leaderboard\",\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gUbiMfNBMKYP"},"outputs":[],"source":["trainer.push_to_hub(**kwargs)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3.10.13 ('myenv')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"vscode":{"interpreter":{"hash":"3cf986cef2fa546fe3f032498b7ba99e6f6a2a5fb52d18405c2d075f2b997d72"}}},"nbformat":4,"nbformat_minor":0}
